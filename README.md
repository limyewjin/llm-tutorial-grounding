# LLM Tutorial - Checking grounding prompts

Using the example from Anthropic's [Prompt Engineering Interactive Tutorial](https://github.com/anthropics/prompt-eng-interactive-tutorial),
I tested the effectiveness of different prompts for avoiding hallucinations.

There are two variables we tested:
1. Whether the prompt includes a request to pull a quote from the document.
2. The order of the document in the prompt.

The results are in `llm_results.txt`.

## Results
